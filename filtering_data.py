# -*- coding: utf-8 -*-
"""filtering_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EF0r1_lXrGWrFhvt7zKOVXf--cMjSNjC

# Retrieving the data

### Mounting the drive
"""

from google.colab import drive 
import os

drive.mount('/content/gdrive')
working_directory = 'My Drive/UnBDC/' 
wd="/content/gdrive/"+working_directory
os.chdir(wd)

dirpath = os.getcwd()
print("current directory is : " + dirpath)

!ls

"""### Retrieving data online, saving it to your drive"""

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# 
# wget -O - "link_to_file" > /content/gdrive/"My Drive"/UnBDC/twitter_data/'file_name'

# Hydrate the tweets using Twitter API and hydrator

"""# Filtering the data"""

import pandas as pd
import matplotlib.pyplot as plt

df1 = pd.read_csv('csv_name.csv')
df2 = pd.read_csv('csv_name_2.csv')

# dropping columns that are empty/irrelevant 

col = ['retweeted_status', 'quote_count', 'reply_count', 'filter_level', 'matching_rules', 'current_user_retweet', 'scopes', 'withheld_copyright', 'withheld_in_countries', 'withheld_scope', 'geo', 'contributors', 'display_text_range', 'quoted_status_permalink']

df1.drop(columns = col, inplace = True)

df2.drop(columns = col, inplace = True)

# sort the tweets so only english tweets are included

df1 = df[df['lang'] == 'en']

df2 = df[df['lang'] == 'en']

# combining our two csv files into one 

dfcomb = pd.concat([df1, df2])
dfcomb.reset_index(inplace = True)
dfcomb.drop(columns = 'index')

# filtering again

dfcomb1 = dfcomb[['created_at', 'full_text', 'place', 'retweet_count' ,'favorite_count', 'possibly_sensitive']]

# switching "created at" into date-time format

dfcomb1['created_at'] = dfcomb1['created_at'].apply(pd.to_datetime)

# save the edited DataFrame as a csv

dfcomb1.to_csv('combined_data.csv')

# adding a column "country"

df3 = pd.read_csv('combined_data.csv', index_col=0)
counter = 0
x = 0 # designating each row's index

df3['country'] = '' # new column

for index, row in df3.iterrows():
    # Select Column with name "Place"
    value = str(row['place'])

    # Change Single Quotes to Double Quotes for JSON
    value = value.strip("'<>() ").replace("\'", '\"')
    counter += 1

    try:
        jsonified = json.loads(value)
        code = jsonified['country_code']

        # adding value to each row's "country" column
        if code == 'US':
          df3.loc[x,'country'] = 'US'
          print(code + " yes")
        else:
          df3.loc[x,'country'] = 'CA'
          print(code + " yes")
    except:
        print(sys.exc_info())

    rowww += 1
    print(counter)

# adding a column "city_state"

counter = 0
x = 0

df3['city_state'] = ''

for index, row in df3.iterrows():
    # Select Column with name "Place"
    value = str(row['place'])

    # Change Single Quotes to Double Quotes for JSON
    value = value.strip("'<>() ").replace("\'", '\"')
    counter += 1

    try:
        jsonified = json.loads(value)
        code = jsonified['full_name']

        # assigning a city,state to column
        df3.loc[x,'city_state'] = code
        print(code + " yes")
    except:
        print(sys.exc_info())

    rowww += 1
    print(counter)

# new data frame with split value columns 
new = df3["city_state"].str.split(",", n = 2, expand = True) 
  
# making separate column from new data frame 
df3["city"]= new[0] 
df3["state"]= new[1] 
  
# Dropping old columns 
df3.drop(columns =["city_state"], inplace = True) 

# saving final DataFrame to csv

df3

df3.to_csv('final_filter.csv')
